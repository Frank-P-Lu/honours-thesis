#+include: "../config.org"

* Burning questions
- Why use the Sigmoid function?
    #+CAPTION: The Logistic Sigmoid function
    #+ATTR_LATEX: :scale 0.2
    [[/home/frank/shared/quantum/honours-thesis/rbm_learning/rbm_learning.org_20220305_105235_N9II2d.png]]

- Multivariate calculus...
    #+ATTR_LATEX: :scale 0.3
    [[/home/frank/shared/quantum/honours-thesis/rbm_learning/rbm_learning.org_20220305_105553_Ghq5ul.png]]

* Very rough notes
The family of gradient based leaning algorithms commonly used by RBMs is called /Contrastive Divergence/.

** The RBM itself
Assume your training set of binary images. Then, you can feed your image \(\mathbb{v}\) into the RBM.

[[/home/frank/shared/quantum/honours-thesis/rbm_learning/rbm_learning.org_20220302_123159_1KYkb9.png]]

Your \(\mathbf{h}\) vectors are binary vectors, and the sum is taken over all possible values of


** Contrastive Divergence
Certain models result in an /unnormalised probability distributions/ (page 582, textbook), where the total probability is not 1.

*** Partition Function
#+begin_export latex
To normalise such a distribution \(\tilde{p}(\mathbf{x})\),

\begin{equation}
p(\mathbf{x}) = \frac{1}{Z} \tilde{p}(\mathbf{x})
\end{equation}
#+end_export

And, \(Z\) which is called the /partition function/ is defined as follows for continuous variables.

#+begin_export latex
\begin{equation}
Z = \int \tilde{p}(\mathbf{x}) d\mathbf{x}
\end{equation}

Or, for discrete variables

\begin{equation}
Z = \sum_{\mathbf{x}} \tilde{p}(\mathbf{x})
\end{equation}
#+end_export

Note that the argument to \(Z\) is often omitted in the literature.

Usually, it is intractable to compute \(Z\) directly, so an approximation is used.

#+begin_export latex
\begin{Notes}
!Gibbs distribution! -- distribution of product of clique potentials.
\end{Notes}
#+end_export

* Machine learning notes
Here, I will briefly give the background for machine learning in the context of RBMs.

Machine learning algorithms are used when it is difficult to directly write an algorithm to do some task. For example, computer vision, or speech recognition.


Mathematically, we want some function \(f\) given some input to produce an output, but \(f\) cannot be readily defined so we opt for a (not necessarily probablistic) but 'good enough' model. In this case, it is sensible to choose a family of functions (?) \(f ( x ; \theta )\) parameterised by \(\theta\) as our model. In a sense, a good model will have a good choice of \(f\), in that \(f\) is able to approximate the distribution of our outputs, and a good choice of \(\theta\).

We will start with supervised learning, where we have a set \(X = \{x_1, x_2, \dots, x_k \}\) of \(k\) inputs and \(Y = \{y_1, y_2, \dots, y_k\} \) outputs. Our goal is to find \(f : X \to \hat{Y}\), where \(\hat{Y} = \{ \hat{y}_1, \hat{y}_2, \dots, \hat{y}_k\}\) and for all \(i\), \( f(x_i) = \(\hat{y}_i \). The hats on the \(Y\)s is to show that this just an approximation for actual \(y\) values. Of course, a good model will be one such that for all \(i\), \(\hat{y}_i \approx y_i\).


/Linear regression/ is a good example of a machine learning algorithm.

** Finding a good parameterisation
What is a good choice of \(\theta\)? It only makes sense to ask this question if we can measure the performance of a particular choice of \(\theta\). To this end, let \(C(f, \theta)\) be the cost function that produces a scalar output. So now, the best choice of \(\theta\) is one that optimises \(C\).

A simple and commonly used cost function is the mean squared error, \(C(Y, \hat{Y}) = \sum_i (y_i - \hat{y}_i)^2 \).



** Mathematical fundamentals
/From scratch/ spends a lot of time talking about calculating gradients to functions.

Suppose, \(f : A \to A \), \(g : (A, A) \to A \). Then, given the map \(Z(x, y) = (f \circ g) (x, y)\), we want to find out how the gradient of the output with respect to the input. More precisely, \(\frac{dZ}{dx}\) and \(\frac{dZ}{dy}\).
(This is done using the chain rule).

The backward pass is precisely the act of finding gradients with respect to the input variables.




* Understanding: Learning for undirected models
** Partition Functions
** Maximum likelihood
"Negative phase is very hard". So naive algorithm isn't used.

*** Contrastive Divergence
